# 应答

1. 制定了哪些规范
- git flow 规范：feature分支用于并行的新功能开发，develop分支会合并feature分支并解决冲突，master分支用于提测上线，上线后会打tag并标注发版日期和内容，需要线上热修复时从tag创建bugfix修复分支，受保护分支的合并都提交mergeRequest由我这边合并。
- java代码规范：引入阿里巴巴Java规范，同时IDEA安装该规范的插件。
- 建库建表规范：库名、表名、字符集、必备列、索引 等
- 代码提交规范：显式添加提交人和有意义描述，当要合并到master时提交merge request，由我进行 code review 然后合并代码。
- CodeReview规范：新同事的代码会进行逐行CodeReview；热修复上线代码也会进行逐行CodeReview。
- 转测规范：提测时需要附带转测提交单，注明：待提测服务、是否需要执行sql变动、是否需要添加mq的topic、是否需要云平台配置操作权限、服务发布是否有依赖关系、解决了哪些bug、受影响范围是多大 等。
- 日志规范：统一格式，以便EFK日志系统抓取。日志中需要使用MDC打印上下文信息，包括traceId，方便日志查询。
- topics规范：按照 平台名-服务名-业务名-子业务名 的格式定义topic，并由统一的topics文档集中管理。
- 端口规范：预先分配各平台的端口范围，当有新的服务时，从对应的范围表中选取未使用的端口。
- 工程结构规范：对单模块服务、多模块服务，在模块名、包名 设计上的一些规范。比如多模块服务，会打包api层以供别的服务发起远程调用。

2. 做了哪些组件的封装
- framework-sdk中的分布式唯一id生成器
    - 使用了“redis生成时间戳和共享自增序列，本地执行bit位移计算ID”来完成。对于同一个时间戳，由 redis lua 脚本中的逻辑来保证所有客户端操作的是同一个自增序列。
    - 方案的坏处是降低了TPS，单个的业务每毫秒只能有1024个可用ID
    - 好处是严格单调递增，实现简单，可快速上线。另外，项目还暂时不存在1秒高于千万TPS的场景。
- uid-generator
    - 提供了百度的uid方案，基于snowflake来实现，是属于client端方案，具体有包括两种实现：
        - 第一种是原生snowflake，workerId由数据库自增主键生成，对时间、机器、序列所占bit位数重新进行了定义。（时间是到2016-05-20的秒数）
        - 第二种是借用了未来时间（启动的时候记录timestamp，后续采用increaseAndGet的方式，而不是取当前时间），由此，可以无视时间的生成id。同时使用RingBuffer环形队列来做id的预填充/定时填充（线程池定时填充、take是懒加载填充）
        - 第一种的最大并发由自增序列所占bit位数决定（如13bit那就是8192），第二种则由cpu、内存决定（一般能达到600万/s的并发）。这两种方案生成的ID都是趋势递增，而不是单调递增。
    - 提供了美团leaf的方案，是属于server端方案，具体有两种实现：
        - 第一种是原生snowflake，workerId有Zookeeper生产，时间、机器、序列所占bit位数可自定义。性能主要靠横向扩展服务集群。
        - 第二种是segment模式，采用预分发的方式生成ID（每个服务启动时都会去db拿固定长度的id列表，这样子多个服务都是以内存对外提供服务，性能高），另外每个segmeng有双buffer，能容忍一定时间内的db不可用。由于强依赖于DB，所以要求实现DB的高可用部署。
    - 我对百度方案做了两方面增强
        - 第一是对其进行spring-boot-starter封装，yml简单的配置下就能接入使用。（定义AutoConfiguration，并配置到spring.factories）
        - 第二是除了原本提供的mysql生成workerId的实现，新增了Zookeeper和Redis生成workerId的实现。（zk靠持久化顺序节点实现，redis靠KeyValue的increase实现[redis需要开启持久化]）
    - 我对美团方案则是进行ConfigurationProperties的定义，使得yml简单配置即可使用，未对其功能作修改。
- feign
    - 链路上下文（HandlerInterceptor、线程间上下文注入[HystrixConcurrencyStrategy、ThreadFactory、TaskDecorator]、RequestInterceptor）
    - 基类异常定义（Generic、Specific、RawJson、RemoteException 等，用于不同场景下使用）
    - 全局异常处理（RestControllerAdvice实现，捕获处理了99%的异常，也提供了stub方法供使用者拦截）
    - 服务间调用异常解析（自定义ErrorDecoder，解析区间Code为自定义的异常；这块儿判断了是否集成hystrix，所以会用conditional注解定义不同的bean）
    - header透传的声明式支持（只需要yml中定义header的名称）
    - 集成那个了slf4j的MDC（会设置traceId和其他上下文信息，这样子全链路的系统日志都会都会打印出这些信息，定位问题会很方便）
- canal
    - 使用了注解的方式监控MySQL中的数据变动
    - 定义了监听注解，使用反射获取到注解及其内容，包装成Registration注册到Registry中
    - 这块儿是仿照mvc中的HandlerInterceptor的注册逻辑，因为逻辑太像了，mvc中是要对uri进行过滤，而此处是要对库名、表名、字段名进行过滤
    - 最后使用的时候就很简单了，添加注解，设置感兴趣的库、表、字段即可，当有新的binlog时，此处就会消费
- redis
    - 封装了分布式锁的实现
    - 抽象Lock接口，对外屏蔽内部实现细节，本质上是提供了facade接口
    - 内部采用的Redisson实现，
        - 原理是使用Lua脚本保证原子性，数据结构是Hash，因为要记录线程及重入次数
        - 另外，redisson还提供了对锁续约的功能，能解决锁超时带来的并发问题
- rocket
    - 定义生产者bean，yml中可配置连接参数
    - 定义消费者bean，自定义listener注解，注解中可配置各类连接参数
    - 对每一个消费者bean执行订阅topic的逻辑 和 消息分发逻辑
    - 需要用事务消息的已经切换成官方的starter了，其他还是用自己封装的。（7/8月份、12月份）
- slf4j
    - 使用aop实现LogBefore、LogAround、LogAfter、LogAfterReturn、LogAfterThrow 几个注解
    - 用来快速对方法添加日志打印，不用关心格式和序列化
- repeat
    - ff
- dingding
    - 接入钉钉机器人，捕获相关异常、堆栈信息和线程信息等，发送到钉钉机器人
- dds
    - mysql动态数据源
    - redis动态数据源
    - 云平台配置商户的数据源信息，dds拉取信息并执行切换数据源的动作

4. 做了哪些架构选型啊？
- rabbitMQ -> rocketMQ，略
- 网关 zuul -> Gateway，非阻塞IO多路复用，性能相比有10倍之差。
- 配置中心 config -> Apollo，有web端的控制界面，操作简单，也经过了诸多企业级项目的验证。
- mysql和es数据双写 -> canal监控binlog，异步写入es。订单服务在初期业务迭代快，双写方案侵入性强，不利于快速迭代，所以使用canal去解耦。
- WebFlux -> WebMVC，响应式编程，编码难度大，不注重线程切换的话很容易阻塞nio线程。因此，多数服务使用WebMVC，少数服务使用WebFlux。
- 链路追踪zipkin -> skywalking，起源是zipkin内存泄漏问题，寻找替代品，了解到skywalking，发现功能上和界面上都强于zipkin。

5. 做了哪些方案输出啊？
- 限流：网关未对移动端请求限流，仅对web端请求限流。由框架自带的redis令牌桶方案实现，策略为每个用户ip的填充速率为10,最大容量为30；服务间调用的限流由hystrix的线程池隔离实现（对于服务内的请求也可以添加hystrixCommand注解来实现限流）。
- 分布式锁：业务中对性能要求高于对一致性的要求，因此选择redis这样的方案而不是zk的方案。抽象lock接口，具体实现由redisson完成，底层有Lua脚本实现，并支持对锁的续约。
- 一致性：一致性要求较高时采用RocketMQ的事务消息，否则使用普通消息
- 主数据同步：canal监控binlog，异步处理数据并同步到别的平台
- 上下文传递：如何在不侵入业务代码的前提下将context在整条链路上传输。服务间通过header传输，服务内通过ThreadLocal传输，但是业务代码里的线程切换会导致ThreadLocal数据丢失，因此自定义了ThreadFactory，其内部对Runnable进行包装。@Aysnc和hystrix的线程池处理稍有不同，实现的效果都一致，使用者只需引入自定义的starter，即可。
- 全局异常处理：定义通用的异常基类，response包装类，服务间通过自定义ErrorDecoder进行错误解析，服务内提供RestControllerAdvice以捕获全局异常，如果程序员不特意捕获处理的话，可以自动将异常逐级抛出到顶层。
- 延时任务：
    - mysql遍历所有库，性能太差
    - RocketMQ的延时消息，只能使用内置的延时level，无法做到任意时间的延时。可以采用分解时间，但是耗性能
    - Redisson框架的分布式延时队列，发布方使用zadd添加任务，然后会有定时任务使用zrangebyscore检测到期任务并执行zrem和rpush将任务转移到目标队列，使用方对目标队列执行blpop以阻塞获取到期任务。[优雅实现延时任务之Redis篇](https://mp.weixin.qq.com/s?__biz=MzAxNjM2MTk0Ng==&mid=2247485026&idx=2&sn=589c4465f99364076ee083bdf60bf987&chksm=9bf4b6d7ac833fc16660ff50f18548c129aae390439ffea33fcf4be7afe833a69a8d07de7c52&scene=21#wechat_redirect) [聊聊redisson的DelayedQueue](https://www.codercto.com/a/28333.html)
    - ZK的Curator框架的分布式延时队列，zk会以子节点形式存储任务列表，有客户端curator读取列表后本地排序再依次比较是否可执行。性能太差，如果一次性有100万任务也会全部拉到本地。zk是个协调服务，不应该用到存储上，所以不适合我们的场景。[优雅实现延时任务之zookeeper篇](https://blog.csdn.net/weixin_34049948/article/details/89660107)
    - 最后的话，跟产品讨论利弊，最终是说服产品在原型设计上使用约定好的延时level，所以最终我们是采用RocketMQ的延时消息方案。

6. 讲一下项目的拓扑图？
- todo
- 阿里云DNS，阿里云SLB，阿里云ECS，K8S管理的Nginx，K8S管理的Gateway，K8S管理的其他微服务

7. 讲一下你的工作职责？
- 架构选型、组件封装、团队组建、业务开发、方案评审、任务分解、项目推动
- 业务开发：从头到尾由我开发的是 厨显服务、打印服务，其他服务要么是从已离职人员处接手，要么是从我手中移交给其他组员。
- 方案评审：产品在内审前会先跟我沟通一次，确定需求列表后，会设计详细需求，最后才集结相关的人员完成外审（为什么要提前沟通呢？因为产品有7、8个，各自负责不同的终端，那么任务会有交叉甚至冲突，所以我会全局地去考虑可完成的需求）
- 任务分解：对于需求，按照服务划分出功能点，交由各服务负责人做方案设计（或者一块儿设计，或者我来设计他执行，取决于具体开发人员的能力），指定其任务完成时间节点。
- 项目推动：根据每天早会组员的昨日总结和今日计划，推动各个需求的实现进度，必要时调整需求优先级。