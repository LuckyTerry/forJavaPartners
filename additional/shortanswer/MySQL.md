# 案例

## SQL优化

**查询制作点状态接口，根据诸多条件筛选出符合条件的记录**

- 连接池：Druid连接池。
- 水平分表：一个商户一个库一张表。使用客户端代理Sharding-JDBC封装的动态数据源框架。
- 垂直分表：表中有一系列涉及到节点时间和操作人的列，大约会占据1/3的列，因此将这部分冷数据拆分成副表。
- 主键索引：单调递增的Long型主键，使用redis按照snowflake算法生成。一方面，单调递增避免页分裂；另一方面，要求主键分布式唯一。
- 覆盖索引：该接口会用到设备的一些信息，为了避免join超过5张表，因此在业务逻辑层单独进行设备查询。为了加快查询效率，为设备表添加了覆盖索引。
- 最左匹配原则：where条件多，使用联合索引，提高查询效率。
- 列区分度原则：将区分读最高的列放在联合索引的最左侧，即deviceId，是比商户/门店 更容易区分的列。
- in关键字的索引匹配：将第一个 in 作为联合索引的最后一个列，将另一个 in 的列拆分为多个查询，并使用 union all 汇总结果行。这样子每一个查询都能走索引。
- 因条件缺省造成的索引匹配失效：在代码层做判断，调用不同的查询语句，避免因缺省值造成联合索引失效。
- 数据量大，单日单商户新增数据可达5W：限定数据的范围，根据apollo可动态调整时间范围，目前是查询24小时以内的数据。超过时间范围的数据将有定时任务更改为完结状态。
- 排序规则：有分页需求，但排序规则很复杂。所以，考虑到对一条记录的读请求远大于写请求，因此在写请求时，更新排序字段的值，以便读请求时能在DB层面做分页，避免读取大量数据到内存中分页。
- explain：对于该接口的各种参数组合进行explain分析，确保都能走索引。
- 定期清理数据：定时任务清理1月前的历史数据。
- 业务层优化：大量使用 spring data cache 对部分数据进行缓存
- 业务层优化：在 aggregation 层使用 webflux（即reactor线程模型的应用）处理请求，减小线程间上下文切换的开销

最终满足单实例稳定TPS>=300

## SQL中遇到的问题

对KDS备菜接口进行压测，发现原本1ms的插入语句有时会执行20ms、甚至200ms以上

- 压测导致数据插入频繁，redo log 迅速被写满，并进行“刷脏页”操作，导致执行变慢。

KDS制作接口，出现获取锁超时

- 制作接口中包含update操作。因为where字段未加索引，导致锁表。刚上线时数据量不够大以至于未察觉该问题，后来数据量大了之后才暴露出该问题。

查询出全表数据

- MybatisPlus的in标签，在Collection为空的时候未拼接sql语句，导致查询出来全表的数据，造成OOM。最后在Isuue上发现了这个问题，作者已经修复，于是升级到最新版本。
- Mybatis的where标签，在传入的Param都为null的情况下，默认不拼接where语句，导致查询出来全表的数据，造成OOM。最后在业务层做了非空校验。
